#Crawler Module
#Kevin Lai 101288231

import webdev
import os
import matmult
import math
num_to_url = []
url_to_num = {}

#Function is the core function and starting point in the application. The seed URL is passed in and a queue is created
#to start the crawl process. Each webpage is only parsed once. The seed URL is the starting point and all outgoing URL
#that have not been crawled yet are added to the queue. The crawl process is done when all webpages have been crawled and
#the data have been stored
#Parameter: seed: the starting URL
#Return: num_pages: number of pages crawled
def crawl(seed):

    delete_directory("crawl_data")

    queue=[]
    num_pages = 0
    page = ""
    title = ""
    fruit_list = []
    url_list = []
    queue.append(seed)
    next_url = ""
    idf_dict = {}

    while(queue):
        new_directory = os.path.join("crawl_data", str(num_pages))

        #creates a directory within the crawl data and the name is indexes starting from 0
        if not os.path.exists(new_directory):
            os.makedirs(new_directory)

        #grabs the next url from the queue to be parsed
        next_url = queue.pop(0)

        #maps the current url to a index
        url_to_num[next_url] = num_pages
        num_to_url.append(next_url)

        #coverts a webpage into a string of html
        page = webdev.read_url(next_url)

        #parse the string of html
        title, fruit_list, url_list = parse_page(page)

        calculate_tf(new_directory,fruit_list,idf_dict)
        add_to_file(new_directory,"title.txt",title)

        for x in url_list:
            if x[0] == ".":
                #slices the url from the start to the last / it encounters
                slice_url = slice(0, next_url.rfind('/'))
                next_url = next_url[slice_url]
                #strip the . off of the relative link
                temp_url = x.lstrip(".")
                #concatenate the original url with the relative link
                next_url += temp_url
            add_to_file(new_directory,"outgoing_links.txt",next_url)
            if next_url not in url_to_num:
                queue.append(next_url)
                url_to_num[next_url] = 0

        num_pages+=1

    save_incoming_links("crawl_data")
    calculate_idf("crawl_data",idf_dict)
    calculate_tf_idf("crawl_data")
    calculate_page_ranks()

    return num_pages

#Function is used to create new files based on the given file path and file name. if the file already
#exists then the content is just appended to the file.
#Parameter: file_path: file path to add new file to: file_name: name of new file: contents: contents to be append to file
def add_to_file(file_path,file_name,contents):
    if os.path.isdir(file_path):
        file_path = os.path.join(file_path, file_name)
        fileout = open(file_path, "a+")
        fileout.write(contents+"\n")
        fileout.close()

#Function uses recursion to delete a directory. If there are files within the directory it will delete them first before deleting
#the directory. The os module is used to delete the files and directory so that the operating system you are running will not be
#affected. All files and directories deleted will not end up in the recycling bin.
#Parameter: starting file path "crawler_data"
def delete_directory(file_path):
    if(os.path.isdir(file_path)):
        for i in os.listdir(file_path):

            new_file_path = os.path.join(file_path, i)

            if os.path.isdir(new_file_path):
                delete_directory(new_file_path)
            else:
                os.remove(new_file_path)

        os.rmdir(file_path)

#Function parses the string representation of the webpage that was generated by the read_url function from the web dev module
#Parameter: page: string representation of a webpage
#Return: title: webpages title: fruit_list:list of unique words from webpage paragraph: url_list: list of outgoing url for webpage
def parse_page(page):
    title_flag = False
    title_find_start = False
    paragraph_flag = False
    paragraph_find_start = False
    url_find_quotations = False
    url_flag = False

    title = ""
    fruit_word = ""
    url_string = ""
    fruit_list = []
    url_list =[]
    i = 0

    while i < len(page):
        if page[i] == "<":
            #check for <title tag
            if page[i+1].lower() == "t":
                if page[i+2].lower() == "i":
                    if page[i+3].lower() == "t":
                        if page[i+4].lower() == "l":
                            if page[i+5].lower() == "e":
                                i+=6
                                title_find_start = True
                                continue
            #check for <a tag
            if page[i+1].lower() == "a":
                url_find_quotations = True
                i+=2
                continue
            #check for <p tag
            if page[i+1].lower() == "p":
                i+=2
                paragraph_find_start = True
                continue

        #parse the title and save it in title variable
        if title_flag:
            if page[i] == "<":
                if page[i+1].lower() == "/":
                    if page[i+2].lower() == "t":
                        if page[i+3].lower() == "i":
                            if page[i+4].lower() == "t":
                                if page[i+5].lower() == "l":
                                    if page[i+6].lower() == "e":
                                        if page[i+7].lower() == ">":
                                            i+=8
                                            title_flag = False
                                            continue
            title+=page[i]
        #parse the paragraph and add each fruit to a list
        if paragraph_flag:
            if page[i] == "<":
                if page[i+1] == "/":
                    if page[i+2] == "p":
                        if page[i+3] == ">":
                            paragraph_flag = False
                            i+=4
                            continue
            if page[i] != "\n" or not page[i].isspace():
                fruit_word += page[i]
            else:
                fruit_list.append(fruit_word)
                fruit_word = ""
        #parse the url and save to a list
        if url_flag:
            if page[i] == '"':
                url_flag = False
                url_list.append(url_string)
                url_string = ""
                continue
            url_string+=page[i]

        #since <title tag has been found need to look for > to find start of title tag, this allows us to skip over the attributes
        if title_find_start:
            if page[i] == '>':
                title_flag = True
                title_find_start = False

        #since <p tag has been found need to look for > to find start of paragraph tag, this allows us to skip over the attributes
        if paragraph_find_start:
            if page[i] == '>':
                if page[i+1] == "\n":
                    i+=1
                paragraph_flag = True
                paragraph_find_start = False

        #since <a tag has been found need to look for href=" to find start of url
        if url_find_quotations:
            if page[i] == 'h':
                if page[i+1] == 'r':
                    if page[i+2] == 'e':
                        if page[i+3] == 'f':
                            if page[i+4] == '=':
                                if page[i+5] == '"':
                                    i+=5
                                    url_flag = True
                                    url_find_quotations = False

        #increment counter
        i+=1

    return title, fruit_list, url_list

#Function loops through all the webpage�s directories and for all outgoing links it creates an incoming link text
#file for those URLs and appends the current webpage URL.
#Parameter: file_path: starting file path "crawler_data"
def save_incoming_links(file_path):
    #loop through all webpage(URLs)
    for key,value in url_to_num.items():
        current_url_file_path = os.path.join(file_path,str(value))
        outgoing_links_path = os.path.join(current_url_file_path, "outgoing_links.txt")
        if os.path.isfile(outgoing_links_path):
            filein = open(outgoing_links_path, "r")
            #loop though all outgoing links and append incoming link as the current url the incoming_links text file
            for x in filein:
                incoming_url_path = os.path.join(file_path, str(url_to_num[x.strip()]))
                add_to_file(incoming_url_path,"incoming_links.txt", key)
#Function calculates the term frequency for each webpage by storing the count of each unique word in a text file
#Parameter: file_path: current webpage directory: paragraph: list of unique words on the current webpage:
#           idf_dict: dictionary that holds the counts for the unique words across all URLs
def calculate_tf(file_path,paragraph,idf_dict):
    fruit_dic = {}
    for x in paragraph:
        if x in fruit_dic:
            fruit_dic[x] +=1
        else:
            fruit_dic[x] = 1
            #calculate idf at the same time as tf by only adding the unique words from the paragraph for that url
            if x in idf_dict:
                idf_dict[x] += 1
            else:
                idf_dict[x] = 1

    new_directory = os.path.join(file_path, "tf")

    if not os.path.exists(new_directory):
        os.makedirs(new_directory)
    #add a file in the tf directory name will be the fruit and content will be the term frequency
    for key, value in fruit_dic.items():
        add_to_file(new_directory,key+".txt",str(value/len(paragraph)))

#Function calculates the inverse document frequency of each unique word contained in all the webpages (URLs) then stores
#the data as text files for each unique word in the idf directory which is in the crawler_data directory.
#Parameter: file_path: starting file path "crawler_data": idf_dict: dictionary that holds the counts for the unique words across all URLs
def calculate_idf(file_path,idf_dict):
    new_directory = os.path.join(file_path, "idf")
    os.mkdir(new_directory)

    #calculate the idf for each of the unique words and store in text files under the idf directory which is in crawler_data directory
    for key, value in idf_dict.items():
        add_to_file(new_directory, key+".txt", str(math.log2(len(url_to_num)/(value+1))))

#Function calculates the tf-idf weight of each unique word in all the webpages (URLs) and stores this data in a directory
#named tf_idf in each of the webpage�s directories.
#Parameter: file_path: starting file path "crawler_data"
def calculate_tf_idf(file_path):
    idf_directory = os.path.join(file_path,"idf")

    #loop through all the webpages
    for values in url_to_num.values():
        new_directory = os.path.join(file_path, str(values))
        tf_idf_directory = os.path.join(new_directory, "tf_idf")
        tf_directory = os.path.join(new_directory, "tf")

        if not os.path.exists(tf_idf_directory):
            os.makedirs(tf_idf_directory)
        #loop though all of the files in the tf directory
        for i in os.listdir(tf_directory):
            tf_file = os.path.join(tf_directory, i)
            idf_file = os.path.join(idf_directory,i)

            if os.path.isfile(tf_file):
                filein = open(tf_file, "r")
                tf = float(filein.readline())
                filein.close()

            if os.path.isfile(idf_file):
                filein = open(idf_file, "r")
                idf = float(filein.readline())
                filein.close()
            #calculate the tf_idf for the current word and store in a text file under the tf_idf directory which is in the current webpages directory
            tf_idf = math.log2(1+(tf))*idf
            add_to_file(tf_idf_directory,i,str(tf_idf))

#Function This function calculates the page rank of every webpage (URLs) following the algorithm for computing page rank from
#the lecture videos. The alpha value used is 0.1
def calculate_page_ranks():
    adjacency_matrix = []
    t = [[]]
    old_t = [[]]
    num_of_ones = 0
    alpha = 0.1

    #creating adjacency matrix
    for i in range(len(url_to_num)):
        t[0].append((1/len(url_to_num)))
        adjacency_matrix.append([0] * len(url_to_num))

        file_path = os.path.join("crawl_data",str(i))
        file = os.path.join(file_path, "outgoing_links.txt")
        if os.path.isfile(file):
            filein = open(file, "r")
            #check outgoing links, for each one find the index corresponding to it and assign 1
            for x in filein:
                adjacency_matrix[i][url_to_num[x.strip()]] = 1
                num_of_ones +=1
        for j in range(len(adjacency_matrix[i])):
            #Initial transition probability matrix (dividing rows by # of 1s):
            if(num_of_ones == 0):
                adjacency_matrix[i][j] = 0
            else:
                adjacency_matrix[i][j]/=num_of_ones
            #Scaled Adjancency Matrix (1-alpha * adjacency)
            adjacency_matrix[i][j]*= (1-alpha)
            #Adding alpha/N
            adjacency_matrix[i][j] += (alpha/len(adjacency_matrix[i]))
        num_of_ones = 0

    #power iteration continues until the euclidean distance between the current vector and previous is less then 0.0001
    while True:
        old_t = t
        t = matmult.mult_matrix(t,adjacency_matrix)
        if matmult.euclidean_dist(t,old_t) <= 0.0001:
           break
    #store the page_rank in a text file under the current webpage directory
    for i in url_to_num.values():
        path = os.path.join("crawl_data",str(i))
        add_to_file(path,"page_rank.txt",str(t[0][i]))
